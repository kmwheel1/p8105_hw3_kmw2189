---
title: "Homework 3"
author: "Kylie Wheelock Riley"
date: "10/8/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Problem 1
```{r importing problem 1 data, include = FALSE}
insta_data = readr::read_csv(file = "./data/instacart.csv.zip") 

##The dataset has 1,384,617 rows of data for 131,206 distinct orders
nrow(insta_data)
n_distinct(pull(insta_data, order_id))


## Looking at individual variables 
#order_id - order ID has a mean of 1706298 and max value of 3421070
mean(pull(insta_data, order_id))
max(pull(insta_data, order_id))

insta_data %>% 
  group_by(order_id) %>% 
  summarize(
    n_obs = n(),
    n_distinct(product_id)
  ) 
 

#product_id - there are 39,123 unique products ordered in this dataset, and product 24852 is the most popular with 18,726 orders. 
n_distinct(pull(insta_data, product_id))

insta_data %>% 
  group_by(product_id) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) 
  

#add_to_cart_order - the maximum add to cart order was 80
max(pull(insta_data, add_to_cart_order))
 


#reordered - 60% of items were reorders and 40% were new. Plot showing this. ## Want to figure out how to plot a percent. 
insta_data %>% 
  group_by(reordered) %>%
  summarize(count = n()) %>%
  mutate(
    percent = round((count/sum(count)*100))
    )


ggplot(insta_data, aes(x = reordered)) + 
  geom_histogram(binwidth = 0.45) +
  labs(
    title = "frequency of items that are reordered",
    x = "Reorder",
    y = "Frequency",
    caption = "Instacart grocery order data."
  ) +
  scale_x_continuous(
    breaks = c(0, 1), 
    labels = c("no", "yes")
    )


#order_hour_of_day - 2 pm is the most pupular time of day to place food orders. 
insta_data %>% 
  group_by(order_hour_of_day) %>%
  summarize(count = n()) %>%
  arrange(desc(count))


#days_since_prior_order - on average people wait 17 days to place their next food order. 
mean(pull(insta_data, days_since_prior_order))


#product_name - produces a table with the top 10 items ordered.  The number 1 ordered item was bananas. 
insta_data %>% 
  group_by(product_name) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  slice(1:10) %>% 
  knitr::kable()

```
The Instacart dataset has `r nrow(insta_data)` rows of data for `r n_distinct(pull(insta_data, order_id))` distinct orders. There were `r n_distinct(pull(insta_data, product_id))` unique products ordered in this dataset, and bananas are the most popular item with 18726 orders. The top 10 items ordered are listed below **Plug Product Table Here**. 60% of the items ordered were reorders **INSERT HIST HERE**. Last but not least, people waited on on average `r mean(pull(insta_data, days_since_prior_order))` days to place this order after their previous order. 


**1A) How many aisles are there, and which aisles are the most items ordered from?**
```{r problem 1 aisle count, include = FALSE}
max(pull(insta_data, aisle_id))
n_distinct(pull(insta_data, aisle_id))
```
There are `r n_distinct(pull(insta_data, aisle_id))` aisles at the Instacart warehouse.  The most popular aisle is Fresh Vegetables, and the top 5 aisles are listed below. 
```{r most popular aisle table, echo = FALSE}
insta_data %>% 
  group_by(aisle) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  slice(1:5) %>%
  knitr::kable() 
```


**1B) Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.**
```{r prob 1B, echo = FALSE}
aisles_over_10000 = 
 insta_data %>% 
  group_by(aisle_id) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  filter(count > 10000) 
 
ggplot(aisles_over_10000, aes(x = aisle_id, y = count)) + 
  geom_point(aes(alpha = .5))

ggplot(aisles_over_10000, aes(x = aisle_id)) + 
  geom_histogram(binwidth = 30) +
  labs(
    title = "Aisles with the most Orders",
    x = "Aisle",
    y = "Frequency",
    caption = "Aisles with over 10,000 products ordered"
  ) 

```


**1C) Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.**
```{r prob 1c, echo = FALSE}
prob_1c_df = 
  insta_data %>% 
   filter(aisle %in% c("baking ingredients" , "dog food care" , "packaged vegetables fruits" ))
  
 
prob_1c_df %>%    
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>% 
  slice(1:3) %>% 
  knitr::kable()

```

**1D) Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**
```{r prob 1D, echo = FALSE}
prob_1d_df = 
  insta_data %>% 
   filter(product_name %in% c("Pink Lady Apples" , "Coffee Ice Cream" )) %>% 
    mutate(
    order_dow = as.factor(order_dow), 
    order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thu", "5" = "Fri", "6" = "Sat")
    ) %>% 
   select(product_name, order_dow, order_hour_of_day)
  
 
prob_1d_df %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
  mean_hour = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(digits = 2)

```


# Problem 2

```{r problem 2 dataset cleaning, include = FALSE}
brfss_data = readr::read_csv(file = "./data/brfss_data.csv.zip") %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr) %>% 
  separate(locationdesc, into = c("state2", "location"), sep = "-") %>% 
  select(-state2) 
##Warning coming up that additional pieses discarded in 500 rows, but the # of obersations and vars are the same. 
brfss_temp = brfss_data %>% 
  group_by(class) %>% 
  filter(topic == "Overall Health") %>%
  mutate( 
    response = as.factor(response),
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))
    ) %>% 
  ungroup()

```


**2A) In 2002, which states were observed at 7 or more locations? What about in 2010?**

In 2002 there were 6 states that had 7 or more loctions.  
```{r 2002 location table, echo = FALSE}
brfss_data %>%
  group_by(state) %>% 
  filter(year == 2002) %>%
  summarize(
    number_of_locations = n_distinct(location)
    ) %>% 
  arrange(desc(number_of_locations)) %>% 
  filter(number_of_locations >= 7) %>% 
  knitr::kable()

```

In 2010 there were 14 states that had 7 or more locations. 
```{r 2010 location table, echo = FALSE}
brfss_data %>%
  group_by(state) %>% 
  filter(year == 2010) %>%
  summarize(
    number_of_locations = n_distinct(location)
    ) %>% 
  arrange(desc(number_of_locations)) %>% 
  filter(number_of_locations >= 7) %>% 
  knitr::kable() 

```

**2B) Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state **
```{r problem 2b, echo = FALSE, warning = FALSE}
prob2b_df = brfss_data %>% 
  group_by(year, state) %>% 
  filter(response == "Excellent") %>% 
  summarize(
    avg_data_value = mean(data_value)
  ) 


ggplot(data = prob2b_df, aes(x = year, y = avg_data_value, group = state, color = state )) + 
  geom_line() +
   labs(
    title = "Health Score for Respondants who Report being in Excellent Health by State",
    x = "Year",
    y = "Average Health Score"
  ) +
  theme(legend.position = "bottom")

```

**2C) Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.**
```{r problem 2c, echo = FALSE}
 brfss_temp %>%
  group_by(location, response) %>% 
  filter(
    year %in% c(2006 , 2010),
    state == "NY") %>% 
  ggplot(aes(x = location, fill = data_value)) +
  geom_density(alpha = .5) + 
  facet_grid(~year) + 
  viridis::scale_fill_viridis(discrete = TRUE) +
  theme(axis.text.x = element_text(angle = 90))
```


# Problem 3

Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).
```{r}
accel_data = readr::read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names() 
```

