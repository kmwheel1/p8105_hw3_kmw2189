---
title: "Homework 3"
author: "Kylie Wheelock Riley"
date: "10/8/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Problem 1
```{r importing problem 1 data, include = FALSE}
insta_data = readr::read_csv(file = "./data/instacart.csv.zip") 

##The dataset has 1,384,617 rows of data for 131,206 distinct orders
nrow(insta_data)
n_distinct(pull(insta_data, order_id))


## Looking at individual variables 
#order_id - order ID has a mean of 1706298 and max value of 3421070
mean(pull(insta_data, order_id))
max(pull(insta_data, order_id))

insta_data %>% 
  group_by(order_id) %>% 
  summarize(
    n_obs = n(),
    n_distinct(product_id)
  ) 
 

#product_id - there are 39,123 unique products ordered in this dataset, and product 24852 is the most popular with 18,726 orders. 
n_distinct(pull(insta_data, product_id))

insta_data %>% 
  group_by(product_id) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) 
  

#add_to_cart_order - the maximum add to cart order was 80
max(pull(insta_data, add_to_cart_order))
 


#reordered - 60% of items were reorders and 40% were new. Plot showing this. ## Want to figure out how to plot a percent. 
insta_data %>% 
  group_by(reordered) %>%
  summarize(count = n()) %>%
  mutate(
    percent = round((count/sum(count)*100))
    )


ggplot(insta_data, aes(x = reordered)) + 
  geom_histogram(binwidth = 0.45) +
  labs(
    title = "frequency of items that are reordered",
    x = "Reorder",
    y = "Frequency",
    caption = "Instacart grocery order data."
  ) +
  scale_x_continuous(
    breaks = c(0, 1), 
    labels = c("no", "yes")
    )


#order_hour_of_day - 2 pm is the most pupular time of day to place food orders. 
insta_data %>% 
  group_by(order_hour_of_day) %>%
  summarize(count = n()) %>%
  arrange(desc(count))


#days_since_prior_order - on average people wait 17 days to place their next food order. 
mean(pull(insta_data, days_since_prior_order))


#product_name - produces a table with the top 10 items ordered.  The number 1 ordered item was bananas. 
insta_data %>% 
  group_by(product_name) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  slice(1:10) %>% 
  knitr::kable()

```
The Instacart dataset has `r nrow(insta_data)` rows of data for `r n_distinct(pull(insta_data, order_id))` distinct orders. There were `r n_distinct(pull(insta_data, product_id))` unique products ordered in this dataset, and bananas are the most popular item with 18726 orders. The top 10 items ordered are listed below **Plug Product Table Here**. 60% of the items ordered were reorders **INSERT HIST HERE**. Last but not least, people waited on on average `r mean(pull(insta_data, days_since_prior_order))` days to place this order after their previous order. 


**1A) How many aisles are there, and which aisles are the most items ordered from?**
```{r problem 1 aisle count, include = FALSE}
max(pull(insta_data, aisle_id))
n_distinct(pull(insta_data, aisle_id))
```
There are `r n_distinct(pull(insta_data, aisle_id))` aisles at the Instacart warehouse.  The most popular aisle is Fresh Vegetables, and the top 5 aisles are listed below. 
```{r most popular aisle table, echo = FALSE}
insta_data %>% 
  group_by(aisle) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  slice(1:5) %>%
  knitr::kable() 
```


**1B) Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.**
```{r prob 1B, echo = FALSE}
aisles_over_10000 = 
 insta_data %>% 
  group_by(aisle_id) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% 
  filter(count > 10000) 
 
ggplot(aisles_over_10000, aes(x = aisle_id, y = count)) + 
  geom_point(aes(alpha = .5))

ggplot(aisles_over_10000, aes(x = aisle_id)) + 
  geom_histogram(binwidth = 30) +
  labs(
    title = "Aisles with the most Orders",
    x = "Aisle",
    y = "Frequency",
    caption = "Aisles with over 10,000 products ordered"
  ) 

```


**1C) Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.**
```{r prob 1c, echo = FALSE}
prob_1c_df = 
  insta_data %>% 
   filter(aisle %in% c("baking ingredients" , "dog food care" , "packaged vegetables fruits" ))
  
 
prob_1c_df %>%    
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>% 
  slice(1:3) %>% 
  knitr::kable()

```

**1D) Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**
```{r prob 1D, echo = FALSE}
prob_1d_df = 
  insta_data %>% 
   filter(product_name %in% c("Pink Lady Apples" , "Coffee Ice Cream" )) %>% 
    mutate(
    order_dow = as.factor(order_dow), 
    order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thu", "5" = "Fri", "6" = "Sat")
    ) %>% 
   select(product_name, order_dow, order_hour_of_day)
  
 
prob_1d_df %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
  mean_hour = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(digits = 2)

```


# Problem 2

```{r problem 2 dataset cleaning, include = FALSE}
brfss_data = readr::read_csv(file = "./data/brfss_data.csv.zip") %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr) %>% 
  separate(locationdesc, into = c("state2", "location"), sep = "-") %>% 
  select(-state2) 
##Warning coming up that additional pieses discarded in 500 rows, but the # of obersations and vars are the same. 
brfss_temp = brfss_data %>% 
  group_by(class) %>% 
  filter(topic == "Overall Health") %>%
  mutate( 
    response = as.factor(response),
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))
    ) %>% 
  ungroup()

```


**2A) In 2002, which states were observed at 7 or more locations? What about in 2010?**

In 2002 there were 6 states that had 7 or more loctions.  
```{r 2002 location table, echo = FALSE}
brfss_data %>%
  group_by(state) %>% 
  filter(year == 2002) %>%
  summarize(
    number_of_locations = n_distinct(location)
    ) %>% 
  arrange(desc(number_of_locations)) %>% 
  filter(number_of_locations >= 7) %>% 
  knitr::kable()

```

In 2010 there were 14 states that had 7 or more locations. 
```{r 2010 location table, echo = FALSE}
brfss_data %>%
  group_by(state) %>% 
  filter(year == 2010) %>%
  summarize(
    number_of_locations = n_distinct(location)
    ) %>% 
  arrange(desc(number_of_locations)) %>% 
  filter(number_of_locations >= 7) %>% 
  knitr::kable() 

```

**2B) Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state **
```{r problem 2b, echo = FALSE, warning = FALSE}
prob2b_df = brfss_data %>% 
  group_by(year, state) %>% 
  filter(response == "Excellent") %>% 
  summarize(
    avg_data_value = mean(data_value)
  ) 


ggplot(data = prob2b_df, aes(x = year, y = avg_data_value, group = state, color = state )) + 
  geom_line() +
   labs(
    title = "Health Score for Respondants who Report being in Excellent Health by State",
    x = "Year",
    y = "Average Health Score"
  ) +
  theme(legend.position = "bottom")

```

**2C) Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.**
```{r problem 2c, echo = FALSE}
##brfss_temp %>%
  ##group_by(location, response) %>% 
  ##filter(
    ##year %in% c(2006 , 2010),
    ##state == "NY") %>% 
  ##ggplot(aes(x = location, y = data_value)) + 
  ##geom_violin(aes(fill = data_value), color = "blue", alpha = .5) + 
  ##stat_summary(fun.y = median, geom = "point", color = "blue", size = 4) +
  ##facet_grid(~year) +
  ##theme(axis.text.x = element_text(angle = 90))

  #ggplot(aes(x = location, fill = data_value)) +
  #geom_density(alpha = .5) + 
  #facet_grid(~year) + 
  #viridis::scale_fill_viridis(discrete = TRUE) +
  #
```


# Problem 3

**3A) Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).**
```{r problem 3 dataset tidying, include = FALSE}
accel_data = readr::read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  rename(
    obs_id = day_id) %>% 
  mutate(
    day = as.factor(day),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")), 
    weekday_var = ifelse(day == "Saturday" | day == "Sunday", 0, 1), 
    week = as.factor(week),
    )%>%
  arrange(week, day) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity_value"
    ) %>% 
  mutate(
    minute = as.numeric(minute)
  )
 
```


```{r describing prob 3 dataset, include = FALSE}
##How many weeks are covered in the dataset

##How many days are covered in the dataset - there are data for 7 distinct days worth of data for each week, for a total of 34 days. 
day_count = accel_data %>% 
  group_by(week) %>% 
  summarize(
    n_distinct(day)
  ) 
  
##What is the average number of minutes with activity 

```
The accelerometer dataset contains data for `r n_distinct(pull(accel_data, week))` weeks, and a total of 34 days. the data is summarized by an activity count (step count) per minute of the day. There is a total of `r nrow(accel_data)` minutes in the dataset and there are variables to indicate the day of week and whether it is a weekday (y/n). 

**3B) Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?**
```{r table for 3B, echo = FALSE}
accel_data %>% 
  group_by(week, day) %>% 
  summarize(
    total_min = sum(activity_value)
  ) %>% 
  pivot_wider(
    names_from = week, 
    values_from = total_min
  ) %>% 
  knitr::kable(digits = 0)

```

Daily values of activity range from 1440 to 685910. Most weekday data have values in the 200000 to 400000 range.  Saturdays seems to be the laziest days with 2 weeks coming in at the 1440 activity level. (or this is when they were charging the accelerometer?)


**3C) Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.**
```{r graph for problem 3c, echo = FALSE}
accel_data %>% 
  group_by(week, day) %>% 
  ggplot(aes(x = minute, y = activity_value, group = day, color = day)) + 
  geom_point(alpha = 0.3) +
   labs(
    title = "Activity Level Throughout the Day",
    x = "Time (hour)",
    y = "Activity Level"
  ) +
    theme(legend.position = "bottom") +
  viridis::scale_color_viridis(
    name = "Day", 
    discrete = TRUE
  ) + 
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200), 
    labels = c("12am", "4am", "8am","12pm", "4pm", "8pm")
    )
```

From this plot there are daily patterns that emerge.  There are spikes in activity around 7am, presumably waking up, 11-12pm, presumably lunch, 4-6pm, leaving work, and 8-10pm, perhaps running errands or getting in a workout. I expected to see more distinct differences between weekends and weekdays, but there is not a strong pattern that emerges, besides the no movement Saturdays, which we saw in the previous table. 